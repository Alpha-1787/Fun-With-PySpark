{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Tracking User Activity\n",
    "In this project we will be getting streams of testing/assessment data from education publishing and assessment companies (e.g. Pearson). In order for analysis by data scientists to be done, the data that is being streamed will be captured using Apache Kakfa, and transformed using Spark for the various purposes (e.g. stream analysis, aggregation and storage within a distributed file system)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started\n",
    "In this section we will be creating the directory for which the project work will be stored. It will contain the necessary docker-compose.yml file needed to create the docker cluster that has Kafka, Zookeeper, and Spark. In addition, we will be importing in a sample file that will be used to publish messages in Kafka.\n",
    "\n",
    "### 1.1. Create project2 directory for the work\n",
    "```\n",
    "mkdir ~/w205/project2\n",
    "```\n",
    "\n",
    "### 1.2. Change into the directory that was just created\n",
    "```\n",
    "cd ~/w205/project2\n",
    "```\n",
    "\n",
    "### 1.3. Copy .yml file to the project2 directory so that we can compose a cluster\n",
    "```\n",
    "cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .\n",
    "```\n",
    "\n",
    "### 1.4. Edit the .yml file\n",
    "Changing the mount directory, and also exposing port 8888 for spark so that jupyter notebook can be run.\n",
    "\n",
    "```\n",
    "vi docker-compose.yml\n",
    "\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    #ports:\n",
    "    #- \"8888:8888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - /home/science/w205:/w205\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - /home/science/w205:/w205\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```\n",
    "\n",
    "## 1.5. import sample data\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/f5bRm4\n",
    "```\n",
    "\n",
    "## 1.6. Check that the .json is there\n",
    "```\n",
    "ls -l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cluster setup\n",
    "In this section, we will be starting up the cluster and creating a default topic within Kafka, so that messages can start being published and consumed prior to being transformed using Spark.\n",
    "\n",
    "### 2.1. Start the docker cluster\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "### 2.2. Create a topic named \"commits\"\n",
    "```\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "  \t--create \\\n",
    "\t--topic commits \\\n",
    "\t--partitions 1 \\\n",
    "\t--replication-factor 1 \\\n",
    "\t--if-not-exists \\\n",
    "\t--zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "### 2.3. Check the topic \"commits\" exists\n",
    "```\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --describe \\\n",
    "    --topic commits \\\n",
    "    --zookeeper zookeeper:32181\n",
    "```\n",
    "### 2.4. Check the directory for hadoop\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "### 2.5. open up log\n",
    "Opening a log window for kafka so that activity can be monitored\n",
    "\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Quick investigation on the data\n",
    "Now that Kafka is setup and ready to publish/consume messages, we will do some preliminary quality check on the sample file that will be used to test out the data pipeline.\n",
    "\n",
    "### 3.1. Check how many items are in the .json file (there are 3280)\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project2/assessment-attempts-20180128-121051-nested.json | jq '. | length'\"\n",
    "```\n",
    "\n",
    "### 3.2. Review the .json in a readable format\n",
    "Execute a bash shell in the mids container and cat the .json file and pipe it through jq so that it's in a readable format\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project2/assessment-attempts-20180128-121051-nested.json | jq '.'\"\n",
    "```\n",
    "\n",
    "### 3.3. Check the keys in the .json\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project2/assessment-attempts-20180128-121051-nested.json | jq 'keys[]'\"\n",
    "```\n",
    "\n",
    "### 3.4. Check the first item in the .json\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project2/assessment-attempts-20180128-121051-nested.json | jq '.[0]'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Publishing messages\n",
    "The sample data looks good, and we are now ready to start publishing messages in Kafka. In this section we will be publishing messages in Kafka, reading the messages into an RDD in spark, and doing some simple transformation, using the RDD, to ensure that the pipeline works. \n",
    "\n",
    "### 4.1. publish messages\n",
    "Execute a bash shell in the mids container to run a microservice. The cat piped into jq. The -P flag  tells it to publish messages. The -t flag gives it the topic name of commits. The kafka:29092 tells it the container name and the port number where kafka is running.\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project2/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t commits && echo 'Produced 3280 messages.'\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Working with Spark\n",
    "\n",
    "### 5.1. Open up jupyter notebook \n",
    "Opening up jupyter notebook for the spark container.\n",
    "\n",
    "```\n",
    "docker-compose exec spark \\\n",
    "  env \\\n",
    "    PYSPARK_DRIVER_PYTHON=jupyter \\\n",
    "    PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' \\\n",
    "  pyspark\n",
    "```\n",
    "\n",
    "### 5.2. create symlink in the spark container\n",
    "Creating symlink so that the jupyter notebook can be saved outside of the spark container\n",
    "\n",
    "```\n",
    "docker-compose exec spark bash -c \"ln -s /w205/project2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Raw Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load messages from the \"commits\" topic into a spark df\n",
    "raw_commits_df = (spark.\n",
    "                  read.\n",
    "                  format(\"kafka\").\n",
    "                  option(\"kafka.bootstrap.servers\", \"kafka:29092\").\n",
    "                  option(\"subscribe\",\"commits\").\n",
    "                  option(\"startingOffsets\", \"earliest\").\n",
    "                  option(\"endingOffsets\", \"latest\").\n",
    "                  load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print schema of the raw commits\n",
    "raw_commits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cache the dataframe\n",
    "raw_commits_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Data Transformation\n",
    "In this section, the raw dataframe will be ongoing a series of transformation so that queries can be against it. Also, a schema is written up instead of inferred so that the data can live under one table instead of multiple. Note that the sequences column will contain nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cast the \"value\" column into string since it's in binary\n",
    "commits_df = raw_commits_df.select(raw_commits_df.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet file to hdfs, make sure to check the parquet file is there\n",
    "commits_df.write.parquet(\"/tmp/commits_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nested schema for the data\n",
    "#to-do in the future - create a recursive function to unroll the schema so that this manual step is not needed\n",
    "\n",
    "options_schema = ArrayType(StructType([StructField('at', StringType()),\n",
    "                                       StructField('checked', BooleanType()),\n",
    "                                       StructField('correct', BooleanType()),\n",
    "                                       StructField('id', StringType()),\n",
    "                                       StructField('submitted', IntegerType())]))\n",
    "\n",
    "questions_schema = ArrayType(StructType([StructField('id', StringType()),\n",
    "                                         StructField('options', options_schema),\n",
    "                                         StructField('user_correct', BooleanType()),\n",
    "                                         StructField('user_incomplete',BooleanType()),\n",
    "                                         StructField('user_result', StringType()),\n",
    "                                         StructField('user_submitted', BooleanType())]))\n",
    "\n",
    "counts_schema = StructType([StructField('all_correct', BooleanType()),\n",
    "                            StructField('correct', IntegerType()),\n",
    "                            StructField('incomplete', IntegerType()),\n",
    "                            StructField('incorrect', IntegerType()),\n",
    "                            StructField('submitted', IntegerType()),\n",
    "                            StructField('total', IntegerType()),\n",
    "                            StructField('unanswered', IntegerType())])\n",
    "\n",
    "sequences_schema = StructType([StructField('attempt', IntegerType()),\n",
    "                               StructField('counts', counts_schema),\n",
    "                               StructField('id', StringType()),\n",
    "                               StructField('questions', questions_schema)])\n",
    "                                         \n",
    "commits_schema = StructType([StructField('base_exam_id', StringType()),\n",
    "                             StructField('certification', StringType()),\n",
    "                             StructField('exam_name', StringType()),\n",
    "                             StructField('keen_created_at', StringType()),\n",
    "                             StructField('keen_id', StringType()),\n",
    "                             StructField('keen_timestamp', StringType()),\n",
    "                             StructField('max_attempts', StringType()),\n",
    "                             StructField('sequences', sequences_schema),\n",
    "                             StructField('started_at', StringType()),\n",
    "                             StructField('user_exam_id', StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse through the json and put it into an rdd\n",
    "extracted_commits_rdd = commits_df.rdd.map(lambda x: Row(**json.loads(x.value)))\n",
    "\n",
    "#impose schema that was written above when creating dataframe\n",
    "extracted_commits_df = spark.createDataFrame(extracted_commits_rdd, commits_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- attempt: integer (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- all_correct: boolean (nullable = true)\n",
      " |    |    |-- correct: integer (nullable = true)\n",
      " |    |    |-- incomplete: integer (nullable = true)\n",
      " |    |    |-- incorrect: integer (nullable = true)\n",
      " |    |    |-- submitted: integer (nullable = true)\n",
      " |    |    |-- total: integer (nullable = true)\n",
      " |    |    |-- unanswered: integer (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- questions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- options: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- at: string (nullable = true)\n",
      " |    |    |    |    |    |-- checked: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- correct: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |    |    |-- submitted: integer (nullable = true)\n",
      " |    |    |    |-- user_correct: boolean (nullable = true)\n",
      " |    |    |    |-- user_incomplete: boolean (nullable = true)\n",
      " |    |    |    |-- user_result: string (nullable = true)\n",
      " |    |    |    |-- user_submitted: boolean (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print schema on the df, to ensure that the nested structure that was imposed looks as expected\n",
    "extracted_commits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#register temp table so it can be queried\n",
    "extracted_commits_df.registerTempTable('commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Basic Data Exploration\n",
    "In this section, exploration on the dataset will be done using spark.sql to ensure that the schema that was imposed onto the dataset is correct. Queries against the nested part of the data (i.e. sequences column) in particular is important, as all the data lives in one table as opposed to creating separate table for each of the nested level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_exam_id</th>\n",
       "      <th>certification</th>\n",
       "      <th>exam_name</th>\n",
       "      <th>keen_created_at</th>\n",
       "      <th>keen_id</th>\n",
       "      <th>keen_timestamp</th>\n",
       "      <th>max_attempts</th>\n",
       "      <th>sequences</th>\n",
       "      <th>started_at</th>\n",
       "      <th>user_exam_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37f0a30a-7464-11e6-aa92-a8667f27e5dc</td>\n",
       "      <td>false</td>\n",
       "      <td>Normal Forms and All That Jazz Master Class</td>\n",
       "      <td>1516717442.735266</td>\n",
       "      <td>5a6745820eb8ab00016be1f1</td>\n",
       "      <td>1516717442.735266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, (False, 2, 1, 1, 4, 4, 0), 5b28a462-7a3b-4...</td>\n",
       "      <td>2018-01-23T14:23:19.082Z</td>\n",
       "      <td>6d4089e4-bde5-4a22-b65f-18bce9ab79c8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37f0a30a-7464-11e6-aa92-a8667f27e5dc</td>\n",
       "      <td>false</td>\n",
       "      <td>Normal Forms and All That Jazz Master Class</td>\n",
       "      <td>1516717377.639827</td>\n",
       "      <td>5a674541ab6b0a0001c6e723</td>\n",
       "      <td>1516717377.639827</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, (False, 1, 2, 1, 4, 4, 0), 5b28a462-7a3b-4...</td>\n",
       "      <td>2018-01-23T14:21:47.505Z</td>\n",
       "      <td>2fec1534-b41f-4419-b741-79d372f05cbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4beeac16-bb83-4d58-83e4-26cdc38f0481</td>\n",
       "      <td>false</td>\n",
       "      <td>The Principles of Microservices</td>\n",
       "      <td>1516738973.653394</td>\n",
       "      <td>5a67999d3ed3e300016ef0f1</td>\n",
       "      <td>1516738973.653394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, (False, 3, 0, 1, 4, 4, 0), b370a3aa-bf9e-4...</td>\n",
       "      <td>2018-01-23T20:22:22.584Z</td>\n",
       "      <td>8edbc8a8-4d26-4292-a5af-ae3f246cb09f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4beeac16-bb83-4d58-83e4-26cdc38f0481</td>\n",
       "      <td>false</td>\n",
       "      <td>The Principles of Microservices</td>\n",
       "      <td>1516738921.1137421</td>\n",
       "      <td>5a6799694fc7c70001034706</td>\n",
       "      <td>1516738921.1137421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, (False, 2, 2, 0, 4, 4, 0), b370a3aa-bf9e-4...</td>\n",
       "      <td>2018-01-23T20:21:10.833Z</td>\n",
       "      <td>c0ee680e-8892-4e64-a7f2-bb576a665dc5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6442707e-7488-11e6-831b-a8667f27e5dc</td>\n",
       "      <td>false</td>\n",
       "      <td>Introduction to Big Data</td>\n",
       "      <td>1516737000.212122</td>\n",
       "      <td>5a6791e824fccd00018c3ff9</td>\n",
       "      <td>1516737000.212122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(1, (False, 3, 0, 1, 4, 4, 0), 04a192c1-4f5c-4...</td>\n",
       "      <td>2018-01-23T19:48:42.477Z</td>\n",
       "      <td>e4525b79-7904-4050-a068-27969b01f6bd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           base_exam_id certification  \\\n",
       "0  37f0a30a-7464-11e6-aa92-a8667f27e5dc         false   \n",
       "1  37f0a30a-7464-11e6-aa92-a8667f27e5dc         false   \n",
       "2  4beeac16-bb83-4d58-83e4-26cdc38f0481         false   \n",
       "3  4beeac16-bb83-4d58-83e4-26cdc38f0481         false   \n",
       "4  6442707e-7488-11e6-831b-a8667f27e5dc         false   \n",
       "\n",
       "                                     exam_name     keen_created_at  \\\n",
       "0  Normal Forms and All That Jazz Master Class   1516717442.735266   \n",
       "1  Normal Forms and All That Jazz Master Class   1516717377.639827   \n",
       "2              The Principles of Microservices   1516738973.653394   \n",
       "3              The Principles of Microservices  1516738921.1137421   \n",
       "4                     Introduction to Big Data   1516737000.212122   \n",
       "\n",
       "                    keen_id      keen_timestamp max_attempts  \\\n",
       "0  5a6745820eb8ab00016be1f1   1516717442.735266          1.0   \n",
       "1  5a674541ab6b0a0001c6e723   1516717377.639827          1.0   \n",
       "2  5a67999d3ed3e300016ef0f1   1516738973.653394          1.0   \n",
       "3  5a6799694fc7c70001034706  1516738921.1137421          1.0   \n",
       "4  5a6791e824fccd00018c3ff9   1516737000.212122          1.0   \n",
       "\n",
       "                                           sequences  \\\n",
       "0  (1, (False, 2, 1, 1, 4, 4, 0), 5b28a462-7a3b-4...   \n",
       "1  (1, (False, 1, 2, 1, 4, 4, 0), 5b28a462-7a3b-4...   \n",
       "2  (1, (False, 3, 0, 1, 4, 4, 0), b370a3aa-bf9e-4...   \n",
       "3  (1, (False, 2, 2, 0, 4, 4, 0), b370a3aa-bf9e-4...   \n",
       "4  (1, (False, 3, 0, 1, 4, 4, 0), 04a192c1-4f5c-4...   \n",
       "\n",
       "                 started_at                          user_exam_id  \n",
       "0  2018-01-23T14:23:19.082Z  6d4089e4-bde5-4a22-b65f-18bce9ab79c8  \n",
       "1  2018-01-23T14:21:47.505Z  2fec1534-b41f-4419-b741-79d372f05cbe  \n",
       "2  2018-01-23T20:22:22.584Z  8edbc8a8-4d26-4292-a5af-ae3f246cb09f  \n",
       "3  2018-01-23T20:21:10.833Z  c0ee680e-8892-4e64-a7f2-bb576a665dc5  \n",
       "4  2018-01-23T19:48:42.477Z  e4525b79-7904-4050-a068-27969b01f6bd  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return query results to a pandas dataframe so it can be displayed nicely\n",
    "spark.sql('SELECT * FROM commits LIMIT 5').toPandas().head()\n",
    "\n",
    "#if working in a shell use below\n",
    "#spark.sql('SELECT * FROM commits LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keen_id</th>\n",
       "      <th>id</th>\n",
       "      <th>attempt</th>\n",
       "      <th>all_correct</th>\n",
       "      <th>correct</th>\n",
       "      <th>incomplete</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>submitted</th>\n",
       "      <th>total</th>\n",
       "      <th>unanswered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a6745820eb8ab00016be1f1</td>\n",
       "      <td>5b28a462-7a3b-42e0-b508-09f3906d1703</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a674541ab6b0a0001c6e723</td>\n",
       "      <td>5b28a462-7a3b-42e0-b508-09f3906d1703</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a67999d3ed3e300016ef0f1</td>\n",
       "      <td>b370a3aa-bf9e-4c10-848a-8ecacbd1d93e</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a6799694fc7c70001034706</td>\n",
       "      <td>b370a3aa-bf9e-4c10-848a-8ecacbd1d93e</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a6791e824fccd00018c3ff9</td>\n",
       "      <td>04a192c1-4f5c-4ac1-91df-3f175996af99</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5a67a0b6852c2a00018891fa</td>\n",
       "      <td>e7110aed-0d08-4cb3-9eca-3eb7caad0256</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5a67b627cc80e60001343664</td>\n",
       "      <td>5251db24-2a6e-4247-ab89-c277fb255405</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5a67ac8cb0a5f400017d9919</td>\n",
       "      <td>066b5326-e547-4dab-ad24-ae751f0059d3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5a67a9ba0600870001247a04</td>\n",
       "      <td>8ac691f8-8c1a-4033-b2e2-44e165775992</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5a67ac54411aed0001da9129</td>\n",
       "      <td>066b5326-e547-4dab-ad24-ae751f0059d3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    keen_id                                    id  attempt  \\\n",
       "0  5a6745820eb8ab00016be1f1  5b28a462-7a3b-42e0-b508-09f3906d1703        1   \n",
       "1  5a674541ab6b0a0001c6e723  5b28a462-7a3b-42e0-b508-09f3906d1703        1   \n",
       "2  5a67999d3ed3e300016ef0f1  b370a3aa-bf9e-4c10-848a-8ecacbd1d93e        1   \n",
       "3  5a6799694fc7c70001034706  b370a3aa-bf9e-4c10-848a-8ecacbd1d93e        1   \n",
       "4  5a6791e824fccd00018c3ff9  04a192c1-4f5c-4ac1-91df-3f175996af99        1   \n",
       "5  5a67a0b6852c2a00018891fa  e7110aed-0d08-4cb3-9eca-3eb7caad0256        1   \n",
       "6  5a67b627cc80e60001343664  5251db24-2a6e-4247-ab89-c277fb255405        1   \n",
       "7  5a67ac8cb0a5f400017d9919  066b5326-e547-4dab-ad24-ae751f0059d3        1   \n",
       "8  5a67a9ba0600870001247a04  8ac691f8-8c1a-4033-b2e2-44e165775992        1   \n",
       "9  5a67ac54411aed0001da9129  066b5326-e547-4dab-ad24-ae751f0059d3        1   \n",
       "\n",
       "   all_correct  correct  incomplete  incorrect  submitted  total  unanswered  \n",
       "0        False        2           1          1          4      4           0  \n",
       "1        False        1           2          1          4      4           0  \n",
       "2        False        3           0          1          4      4           0  \n",
       "3        False        2           2          0          4      4           0  \n",
       "4        False        3           0          1          4      4           0  \n",
       "5         True        5           0          0          5      5           0  \n",
       "6         True        1           0          0          1      1           0  \n",
       "7         True        5           0          0          5      5           0  \n",
       "8         True        4           0          0          4      4           0  \n",
       "9        False        0           1          0          1      5           4  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic queries to explore nested data structure\n",
    "sql_query = '''\n",
    "SELECT\n",
    "    keen_id,\n",
    "    sequences.id,\n",
    "    sequences.attempt,\n",
    "    sequences.counts.*\n",
    "FROM\n",
    "    commits\n",
    "LIMIT\n",
    "    10\n",
    "'''\n",
    "\n",
    "#show query results\n",
    "spark.sql(sql_query).toPandas().head(10)\n",
    "\n",
    "#if working in a shell use below\n",
    "#spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keen_id</th>\n",
       "      <th>id</th>\n",
       "      <th>attempt</th>\n",
       "      <th>questions</th>\n",
       "      <th>sequences.questions AS `questions`[0].options</th>\n",
       "      <th>sequences.questions AS `questions`[0].options.correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a6745820eb8ab00016be1f1</td>\n",
       "      <td>5b28a462-7a3b-42e0-b508-09f3906d1703</td>\n",
       "      <td>1</td>\n",
       "      <td>[(7a2ed6d3-f492-49b3-b8aa-d080a8aad986, [Row(a...</td>\n",
       "      <td>[(2018-01-23T14:23:24.670Z, True, True, 49c574...</td>\n",
       "      <td>[True, True, True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a674541ab6b0a0001c6e723</td>\n",
       "      <td>5b28a462-7a3b-42e0-b508-09f3906d1703</td>\n",
       "      <td>1</td>\n",
       "      <td>[(95194331-ac43-454e-83de-ea8913067055, [Row(a...</td>\n",
       "      <td>[(None, False, None, 62feee6e-9b76-4123-bd9e-c...</td>\n",
       "      <td>[None, True, None, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a67999d3ed3e300016ef0f1</td>\n",
       "      <td>b370a3aa-bf9e-4c10-848a-8ecacbd1d93e</td>\n",
       "      <td>1</td>\n",
       "      <td>[(b9ff2e88-cf9d-4bd4-bcea-c2673779425c, [Row(a...</td>\n",
       "      <td>[(2018-01-23T20:22:39.089Z, True, None, c31572...</td>\n",
       "      <td>[None, True, True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a6799694fc7c70001034706</td>\n",
       "      <td>b370a3aa-bf9e-4c10-848a-8ecacbd1d93e</td>\n",
       "      <td>1</td>\n",
       "      <td>[(1f7c5def-904b-4834-8519-639a3b22a496, [Row(a...</td>\n",
       "      <td>[(None, False, None, 429b03ec-bda9-4d4a-bd7f-e...</td>\n",
       "      <td>[None, True, None, True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a6791e824fccd00018c3ff9</td>\n",
       "      <td>04a192c1-4f5c-4ac1-91df-3f175996af99</td>\n",
       "      <td>1</td>\n",
       "      <td>[(620c924f-6bd8-11e6-bcbd-a8667f27e5dc, [Row(a...</td>\n",
       "      <td>[(2018-01-23T19:48:59.018Z, True, None, 5a3ba5...</td>\n",
       "      <td>[None, True, True, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5a67a0b6852c2a00018891fa</td>\n",
       "      <td>e7110aed-0d08-4cb3-9eca-3eb7caad0256</td>\n",
       "      <td>1</td>\n",
       "      <td>[(fb07b16e-84a2-4654-a1d2-eafaabd9b747, [Row(a...</td>\n",
       "      <td>[(None, False, None, c464fd9f-923b-4b68-9eac-f...</td>\n",
       "      <td>[None, True, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5a67b627cc80e60001343664</td>\n",
       "      <td>5251db24-2a6e-4247-ab89-c277fb255405</td>\n",
       "      <td>1</td>\n",
       "      <td>[(247b4589-7f8c-4a46-905c-63e0fa4731af, [Row(a...</td>\n",
       "      <td>[(None, False, None, 9e54b46b-779c-40e6-9fbd-6...</td>\n",
       "      <td>[None, None, True, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5a67ac8cb0a5f400017d9919</td>\n",
       "      <td>066b5326-e547-4dab-ad24-ae751f0059d3</td>\n",
       "      <td>1</td>\n",
       "      <td>[(fc3bdc54-04a8-4b46-976b-43c15bb96e1b, [Row(a...</td>\n",
       "      <td>[(None, False, None, 47363862-3455-4975-ad3c-9...</td>\n",
       "      <td>[None, None, True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5a67a9ba0600870001247a04</td>\n",
       "      <td>8ac691f8-8c1a-4033-b2e2-44e165775992</td>\n",
       "      <td>1</td>\n",
       "      <td>[(803fc93f-7eb2-4121-af8c-ff809e850f10, [Row(a...</td>\n",
       "      <td>[(None, False, None, ba4b7f92-44e1-49ff-8eec-c...</td>\n",
       "      <td>[None, None, True, None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5a67ac54411aed0001da9129</td>\n",
       "      <td>066b5326-e547-4dab-ad24-ae751f0059d3</td>\n",
       "      <td>1</td>\n",
       "      <td>[(fc3bdc54-04a8-4b46-976b-43c15bb96e1b, [Row(a...</td>\n",
       "      <td>[(None, False, True, 81f2a8cc-8143-4d84-9d1b-4...</td>\n",
       "      <td>[True, None, None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    keen_id                                    id  attempt  \\\n",
       "0  5a6745820eb8ab00016be1f1  5b28a462-7a3b-42e0-b508-09f3906d1703        1   \n",
       "1  5a674541ab6b0a0001c6e723  5b28a462-7a3b-42e0-b508-09f3906d1703        1   \n",
       "2  5a67999d3ed3e300016ef0f1  b370a3aa-bf9e-4c10-848a-8ecacbd1d93e        1   \n",
       "3  5a6799694fc7c70001034706  b370a3aa-bf9e-4c10-848a-8ecacbd1d93e        1   \n",
       "4  5a6791e824fccd00018c3ff9  04a192c1-4f5c-4ac1-91df-3f175996af99        1   \n",
       "5  5a67a0b6852c2a00018891fa  e7110aed-0d08-4cb3-9eca-3eb7caad0256        1   \n",
       "6  5a67b627cc80e60001343664  5251db24-2a6e-4247-ab89-c277fb255405        1   \n",
       "7  5a67ac8cb0a5f400017d9919  066b5326-e547-4dab-ad24-ae751f0059d3        1   \n",
       "8  5a67a9ba0600870001247a04  8ac691f8-8c1a-4033-b2e2-44e165775992        1   \n",
       "9  5a67ac54411aed0001da9129  066b5326-e547-4dab-ad24-ae751f0059d3        1   \n",
       "\n",
       "                                           questions  \\\n",
       "0  [(7a2ed6d3-f492-49b3-b8aa-d080a8aad986, [Row(a...   \n",
       "1  [(95194331-ac43-454e-83de-ea8913067055, [Row(a...   \n",
       "2  [(b9ff2e88-cf9d-4bd4-bcea-c2673779425c, [Row(a...   \n",
       "3  [(1f7c5def-904b-4834-8519-639a3b22a496, [Row(a...   \n",
       "4  [(620c924f-6bd8-11e6-bcbd-a8667f27e5dc, [Row(a...   \n",
       "5  [(fb07b16e-84a2-4654-a1d2-eafaabd9b747, [Row(a...   \n",
       "6  [(247b4589-7f8c-4a46-905c-63e0fa4731af, [Row(a...   \n",
       "7  [(fc3bdc54-04a8-4b46-976b-43c15bb96e1b, [Row(a...   \n",
       "8  [(803fc93f-7eb2-4121-af8c-ff809e850f10, [Row(a...   \n",
       "9  [(fc3bdc54-04a8-4b46-976b-43c15bb96e1b, [Row(a...   \n",
       "\n",
       "       sequences.questions AS `questions`[0].options  \\\n",
       "0  [(2018-01-23T14:23:24.670Z, True, True, 49c574...   \n",
       "1  [(None, False, None, 62feee6e-9b76-4123-bd9e-c...   \n",
       "2  [(2018-01-23T20:22:39.089Z, True, None, c31572...   \n",
       "3  [(None, False, None, 429b03ec-bda9-4d4a-bd7f-e...   \n",
       "4  [(2018-01-23T19:48:59.018Z, True, None, 5a3ba5...   \n",
       "5  [(None, False, None, c464fd9f-923b-4b68-9eac-f...   \n",
       "6  [(None, False, None, 9e54b46b-779c-40e6-9fbd-6...   \n",
       "7  [(None, False, None, 47363862-3455-4975-ad3c-9...   \n",
       "8  [(None, False, None, ba4b7f92-44e1-49ff-8eec-c...   \n",
       "9  [(None, False, True, 81f2a8cc-8143-4d84-9d1b-4...   \n",
       "\n",
       "  sequences.questions AS `questions`[0].options.correct  \n",
       "0                                 [True, True, True]     \n",
       "1                           [None, True, None, None]     \n",
       "2                                 [None, True, True]     \n",
       "3                           [None, True, None, True]     \n",
       "4                           [None, True, True, None]     \n",
       "5                                 [None, True, None]     \n",
       "6                           [None, None, True, None]     \n",
       "7                                 [None, None, True]     \n",
       "8                           [None, None, True, None]     \n",
       "9                                 [True, None, None]     "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "SELECT\n",
    "    keen_id,\n",
    "    sequences.id,\n",
    "    sequences.attempt,\n",
    "    sequences.questions,\n",
    "    sequences.questions[0].options,\n",
    "    sequences.questions[0].options.correct\n",
    "FROM\n",
    "    commits\n",
    "LIMIT\n",
    "    10\n",
    "'''\n",
    "\n",
    "#show query results\n",
    "spark.sql(sql_query).toPandas().head(10)\n",
    "\n",
    "#if working in a shell use below\n",
    "#spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Writing out parquet files to HDFS\n",
    "In this section results of some queries will be written out to a parquet file in hdfs. We will be writing out the transformed data set (i.e. extracted_commits_df) onto parquet so that it can be used used by data scientists for their work. In addition we will be doing some aggregation and writing those results onto parquet files so that they can be used for dashboard and/or reporting purposes by the business intelligence analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out the unaggregated transformed commits so that it can be used by data scientists \n",
    "extracted_commits_df.write.parquet('tmp/transformed_commits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_name</th>\n",
       "      <th>number_of_exams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning Git</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to Python</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Java 8</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate Python Programming</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning to Program with R</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Introduction to Machine Learning</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Software Architecture Fundamentals Understandi...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beginning C# Programming</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning Eclipse</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Learning Apache Maven</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           exam_name  number_of_exams\n",
       "0                                       Learning Git              394\n",
       "1                             Introduction to Python              162\n",
       "2                             Introduction to Java 8              158\n",
       "3                    Intermediate Python Programming              158\n",
       "4                         Learning to Program with R              128\n",
       "5                   Introduction to Machine Learning              119\n",
       "6  Software Architecture Fundamentals Understandi...              109\n",
       "7                           Beginning C# Programming               95\n",
       "8                                   Learning Eclipse               85\n",
       "9                              Learning Apache Maven               80"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "SELECT\n",
    "    exam_name,\n",
    "    COUNT(*) number_of_exams\n",
    "FROM\n",
    "    commits\n",
    "GROUP BY\n",
    "    exam_name\n",
    "ORDER BY\n",
    "    number_of_exams DESC\n",
    "'''\n",
    "\n",
    "#write out the query results onto parquet file\n",
    "agg_exam_count = spark.sql(sql_query)\n",
    "agg_exam_count.write.parquet('/tmp/agg_exam_count')\n",
    "\n",
    "#show top 10 rows of the aggregated data\n",
    "spark.sql(sql_query).toPandas().head(10)\n",
    "\n",
    "#if working in a shell use below\n",
    "#spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_name</th>\n",
       "      <th>number_of_exams</th>\n",
       "      <th>avg_incorrects</th>\n",
       "      <th>avg_corrects</th>\n",
       "      <th>avg_unanswered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning Git</td>\n",
       "      <td>394</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to Python</td>\n",
       "      <td>162</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Java 8</td>\n",
       "      <td>158</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate Python Programming</td>\n",
       "      <td>158</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning to Program with R</td>\n",
       "      <td>128</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Introduction to Machine Learning</td>\n",
       "      <td>119</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Software Architecture Fundamentals Understandi...</td>\n",
       "      <td>109</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beginning C# Programming</td>\n",
       "      <td>95</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning Eclipse</td>\n",
       "      <td>85</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Learning Apache Maven</td>\n",
       "      <td>80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           exam_name  number_of_exams  \\\n",
       "0                                       Learning Git              394   \n",
       "1                             Introduction to Python              162   \n",
       "2                             Introduction to Java 8              158   \n",
       "3                    Intermediate Python Programming              158   \n",
       "4                         Learning to Program with R              128   \n",
       "5                   Introduction to Machine Learning              119   \n",
       "6  Software Architecture Fundamentals Understandi...              109   \n",
       "7                           Beginning C# Programming               95   \n",
       "8                                   Learning Eclipse               85   \n",
       "9                              Learning Apache Maven               80   \n",
       "\n",
       "   avg_incorrects  avg_corrects  avg_unanswered  \n",
       "0            1.32          3.38            0.30  \n",
       "1            1.13          2.83            0.26  \n",
       "2            0.56          4.38            0.06  \n",
       "3            1.46          2.05            0.17  \n",
       "4            1.81          3.81            0.18  \n",
       "5            0.83          2.75            0.01  \n",
       "6            1.41          1.92            0.29  \n",
       "7            1.02          2.22            0.08  \n",
       "8            0.81          3.53            0.08  \n",
       "9            1.00          2.44            0.16  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "SELECT\n",
    "    exam_name,\n",
    "    COUNT(*) number_of_exams,\n",
    "    ROUND(AVG(sequences.counts.incorrect), 2) avg_incorrects,\n",
    "    ROUND(AVG(sequences.counts.correct), 2) avg_corrects,\n",
    "    ROUND(AVG(sequences.counts.unanswered), 2) avg_unanswered\n",
    "FROM\n",
    "    commits\n",
    "GROUP BY\n",
    "    exam_name\n",
    "ORDER BY\n",
    "    number_of_exams DESC\n",
    "'''\n",
    "\n",
    "#write out the query results onto parquet file\n",
    "agg_exam_performance = spark.sql(sql_query)\n",
    "agg_exam_performance.write.parquet('/tmp/agg_exam_performance')\n",
    "\n",
    "#show exams by count, avg number of incorrects, avg number of corrects\n",
    "spark.sql(sql_query).toPandas().head(10)\n",
    "\n",
    "#if working in a shell use below\n",
    "#spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary and Closing\n",
    "We have thus far successfully created the cluster needed for the project work, imported the necessary file for testing, published messages via Kafka, and transformed the messages that was published using Spark, and finally outputting the transformed messages into a parquet file in hdfs. Now we will close out of Spark, tear down the cluster, prior to ending the session.\n",
    "\n",
    "### 6.1. Check Files\n",
    "Check that the parquet files that was written out to hdfs.\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/commits_df/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/agg_exam_count/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/agg_exam_performance/\n",
    "```\n",
    "\n",
    "### 6.2. exit out of spark/jupyter notebook\n",
    "```python\n",
    "exit()\n",
    "```\n",
    "\n",
    "### 6.3. Tear down the docker cluster\n",
    "```\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "### 6.4. Check that the cluster is no longer running\n",
    "```\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "### 6.5. Future Enhancements\n",
    "In this assignment I manually wrote down the schema for the data structure so that it can be imposed on the datafram, so that everything can be stored within one table. The type of work I did worked for this assignment, but would be a very tedious thing to do, as there may be more than one data source, so creating a function that will create a schema of a nested data structure that can be imposed on the data source would be nice to have as it can automate the work that I did."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
